{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements for running this notebook:\n",
    "- Python =>3.6 (using f-Strings)\n",
    "- PyTorch => 1.0\n",
    "- scikit-learn\n",
    "- NumPy\n",
    "- PyTables\n",
    "- mt_data.h5 file: You can generate by yourself with this other notebook or [download it](http://ftp.ebi.ac.uk/pub/databases/chembl/blog/pytorch_mtl/mt_data.h5)\n",
    "\n",
    "##Â This notebook trains and test a multi-task neural network on ChEMBL data\n",
    "- It uses a simple shuffled 80/20 train/test split\n",
    "- Automatically configures the output layer no matter the number of targets in the training data.\n",
    "- Tries to use GPU if available\n",
    "- Saves and loads a model to/from a file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as D\n",
    "import tables as tb\n",
    "from sklearn.metrics import (matthews_corrcoef, \n",
    "                             confusion_matrix, \n",
    "                             f1_score, \n",
    "                             roc_auc_score,\n",
    "                             accuracy_score,\n",
    "                             roc_auc_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the device to GPU if available\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set some config values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PATH = '.'\n",
    "DATA_FILE = 'mt_data.h5'\n",
    "MODEL_FILE = 'chembl_mt.model'\n",
    "N_WORKERS = 8 # Dataloader workers, prefetch data in parallel to have it ready for the model after each batch train\n",
    "BATCH_SIZE = 32 # https://twitter.com/ylecun/status/989610208497360896?lang=es\n",
    "LR = 2 # Learning rate. Big value because of the way we are weighting the targets\n",
    "N_EPOCHS = 2 # You should train longer!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the dataset loaders\n",
    "\n",
    "Simple 80/20 train/test split for the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChEMBLDataset(D.Dataset):\n",
    "    \n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        with tb.open_file(self.file_path, mode='r') as t_file:\n",
    "            self.length = t_file.root.fps.shape[0]\n",
    "            self.n_targets = t_file.root.labels.shape[1]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        with tb.open_file(self.file_path, mode='r') as t_file:\n",
    "            structure = t_file.root.fps[index]\n",
    "            labels = t_file.root.labels[index]\n",
    "        return structure, labels\n",
    "\n",
    "\n",
    "dataset = ChEMBLDataset(f\"{MAIN_PATH}/{DATA_FILE}\")\n",
    "validation_split = .2\n",
    "random_seed= 42\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "np.random.shuffle(indices)\n",
    "train_indices, test_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = D.sampler.SubsetRandomSampler(train_indices)\n",
    "test_sampler = D.sampler.SubsetRandomSampler(test_indices)\n",
    "\n",
    "# dataloaders can prefetch the next batch if using n workers while\n",
    "# the model is tranining\n",
    "train_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           num_workers=N_WORKERS,\n",
    "                                           sampler=train_sampler)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset, \n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          num_workers=N_WORKERS,\n",
    "                                          sampler=test_sampler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model, the optimizer and the loss criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChEMBLMultiTask(nn.Module):\n",
    "    \"\"\"\n",
    "    Architecture borrowed from: https://arxiv.org/abs/1502.02072\n",
    "    \"\"\"\n",
    "    def __init__(self, n_tasks):\n",
    "        super(ChEMBLMultiTask, self).__init__()\n",
    "        self.n_tasks = n_tasks\n",
    "        self.fc1 = nn.Linear(1024, 2000)\n",
    "        self.fc2 = nn.Linear(2000, 100)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "        # add an independet output for each task int the output laer\n",
    "        for n_m in range(self.n_tasks):\n",
    "            self.add_module(f\"y{n_m}o\", nn.Linear(100, 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = self.dropout(F.relu(self.fc1(x)))\n",
    "        h2 = F.relu(self.fc2(h1))\n",
    "        out = [torch.sigmoid(getattr(self, f\"y{n_m}o\")(h2)) for n_m in range(self.n_tasks)]\n",
    "        return out\n",
    "    \n",
    "# create the model, to GPU if available\n",
    "model = ChEMBLMultiTask(dataset.n_targets).to(device)\n",
    "\n",
    "# binary cross entropy\n",
    "# each task loss is weighted inversely proportional to its number of datapoints, borrowed from:\n",
    "# http://www.bioinf.at/publications/2014/NIPS2014a.pdf\n",
    "with tb.open_file(f\"{MAIN_PATH}/{DATA_FILE}\", mode='r') as t_file:\n",
    "    weights = torch.tensor(t_file.root.weights[:])\n",
    "    weights = weights.to(device)\n",
    "\n",
    "criterion = [nn.BCELoss(weight=w) for x, w in zip(range(dataset.n_targets), weights.float())]\n",
    "\n",
    "# stochastic gradient descend as an optimiser\n",
    "optimizer = torch.optim.SGD(model.parameters(), LR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "Given the extremely sparse nature of the dataset is difficult to clearly see how the loss is improving after every batch. It looks clearer after several epochs and much more clear when testing :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/2], Step: [500/17789], Loss: 0.01780553348362446\n",
      "Epoch: [1/2], Step: [1000/17789], Loss: 0.01136045902967453\n",
      "Epoch: [1/2], Step: [1500/17789], Loss: 0.018664617091417313\n",
      "Epoch: [1/2], Step: [2000/17789], Loss: 0.013626799918711185\n",
      "Epoch: [1/2], Step: [2500/17789], Loss: 0.012855792418122292\n",
      "Epoch: [1/2], Step: [3000/17789], Loss: 0.013796127401292324\n",
      "Epoch: [1/2], Step: [3500/17789], Loss: 0.021601887419819832\n",
      "Epoch: [1/2], Step: [4000/17789], Loss: 0.00950919184833765\n",
      "Epoch: [1/2], Step: [4500/17789], Loss: 0.02028888650238514\n",
      "Epoch: [1/2], Step: [5000/17789], Loss: 0.013251284137368202\n",
      "Epoch: [1/2], Step: [5500/17789], Loss: 0.008788244798779488\n",
      "Epoch: [1/2], Step: [6000/17789], Loss: 0.012066680938005447\n",
      "Epoch: [1/2], Step: [6500/17789], Loss: 0.013928443193435669\n"
     ]
    }
   ],
   "source": [
    "# model is by default in train mode. Training can be resumed after .eval() but needs to be set to .train() again\n",
    "model.train()\n",
    "for ep in range(N_EPOCHS):\n",
    "    for i, (fps, labels) in enumerate(train_loader):\n",
    "        # move it to GPU if available\n",
    "        fps, labels = fps.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(fps)\n",
    "        \n",
    "        # calc the loss\n",
    "        loss = torch.tensor(0.0).to(device)\n",
    "        for j, crit in enumerate(criterion):\n",
    "            # mask keeping labeled molecules for each task\n",
    "            mask = labels[:, j] >= 0.0\n",
    "            if len(labels[:, j][mask]) != 0:\n",
    "                # the loss is the sum of each task/target loss.\n",
    "                # there are labeled samples for this task, so we add it's loss\n",
    "                loss += crit(outputs[j][mask], labels[:, j][mask].view(-1, 1))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 500 == 0:\n",
    "            print(f\"Epoch: [{ep+1}/{N_EPOCHS}], Step: [{i+1}/{len(train_indices)//BATCH_SIZE}], Loss: {loss.item()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trues = []\n",
    "y_preds = []\n",
    "y_preds_proba = []\n",
    "\n",
    "# do not track history\n",
    "with torch.no_grad():\n",
    "    for fps, labels in test_loader:\n",
    "        # move it to GPU if available\n",
    "        fps, labels = fps.to(device), labels.to(device)\n",
    "        # set model to eval, so will not use the dropout layer\n",
    "        model.eval()\n",
    "        outputs = model(fps)\n",
    "        for j, out in enumerate(outputs):\n",
    "            mask = labels[:, j] >= 0.0\n",
    "            y_pred = torch.where(out[mask] > 0.5, torch.ones(1), torch.zeros(1)).view(1, -1)\n",
    "\n",
    "            if y_pred.shape[1] > 0:\n",
    "                for l in labels[:, j][mask].long().tolist():\n",
    "                    y_trues.append(l)\n",
    "                for p in y_pred.view(-1, 1).tolist():\n",
    "                    y_preds.append(int(p[0]))\n",
    "                for p in out[mask].view(-1, 1).tolist():\n",
    "                    y_preds_proba.append(float(p[0]))\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_trues, y_preds).ravel()\n",
    "sens = tp / (tp + fn)\n",
    "spec = tn / (tn + fp)\n",
    "prec = tp / (tp + fp)\n",
    "f1 = f1_score(y_trues, y_preds)\n",
    "acc = accuracy_score(y_trues, y_preds)\n",
    "mcc = matthews_corrcoef(y_trues, y_preds)\n",
    "auc = roc_auc_score(y_trues, y_preds_proba)\n",
    "\n",
    "print(f\"accuracy: {acc}, auc: {auc}, sens: {sens}, spec: {spec}, prec: {prec}, mcc: {mcc}, f1: {f1}\")\n",
    "print(f\"Not bad for only {N_EPOCHS} epochs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"./{MODEL_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChEMBLMultiTask(560) # number of tasks\n",
    "model.load_state_dict(torch.load(f\"./{MODEL_FILE}\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
